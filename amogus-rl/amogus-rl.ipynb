{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "import concurrent.futures\n",
    "from torch import optim\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import env\n",
    "import network\n",
    "import player\n",
    "\n",
    "\n",
    "BOARD_XSIZE = env.BOARD_XSIZE\n",
    "BOARD_YSIZE = env.BOARD_YSIZE\n",
    "\n",
    "DIMS=(BOARD_XSIZE,BOARD_YSIZE)\n",
    "\n",
    "\n",
    "EPISODES_PER_AGENT = 32\n",
    "TRAIN_EPOCHS = 500000\n",
    "MODEL_SAVE_INTERVAL = 100\n",
    "MAKE_OPPONENT_INTERVAL = 1000\n",
    "SUMMARY_STATS_INTERVAL = 10\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "SUMMARY_DIR = './summary'\n",
    "MODEL_DIR = './models'\n",
    "\n",
    "# create result directory\n",
    "if not os.path.exists(SUMMARY_DIR):\n",
    "    os.makedirs(SUMMARY_DIR)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "cuda = torch.device(\"cuda\")\n",
    "cpu = torch.device(\"cpu\")\n",
    "\n",
    "if use_cuda:\n",
    "    device = cuda\n",
    "else:\n",
    "    device = cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: restore neural net parameters\n",
    "\n",
    "impostor_actor = network.Actor().to(device)\n",
    "impostor_critic = network.Critic().to(device)\n",
    "impostor_actor_optimizer = optim.Adam(impostor_actor.parameters(), lr=network.ACTOR_LR)\n",
    "impostor_critic_optimizer = optim.Adam(impostor_critic.parameters(), lr=network.CRITIC_LR)\n",
    "\n",
    "crewmate_actor = network.Actor().to(device)\n",
    "crewmate_critic = network.Critic().to(device)\n",
    "crewmate_actor_optimizer = optim.Adam(crewmate_actor.parameters(), lr=network.ACTOR_LR)\n",
    "crewmate_critic_optimizer = optim.Adam(crewmate_critic.parameters(), lr=network.CRITIC_LR)\n",
    "\n",
    "# Get Writer\n",
    "writer = SummaryWriter(log_dir=SUMMARY_DIR)\n",
    "\n",
    "impostor_step = 0\n",
    "crewmate_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_against_configuration:dict[tuple[str, str, str], list[float]] = {}\n",
    "\n",
    "\n",
    "player_pool: list[player.Player] = [\n",
    "    player.RandomPlayer(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_valid_location() -> tuple[int, int]:\n",
    "    x = np.random.randint(0, BOARD_XSIZE)\n",
    "    y = np.random.randint(0, BOARD_YSIZE)\n",
    "    return (x, y)\n",
    "\n",
    "\n",
    "def play(actor_engine: player.ActorPlayer, actor_is_impostor: bool, other_engines: list[player.Player]) -> tuple[\n",
    "    list[env.Observation],\n",
    "    list[env.Action],\n",
    "    list[np.ndarray],\n",
    "    list[env.Reward],\n",
    "    list[env.Advantage],\n",
    "    list[env.Reward],\n",
    "    bool\n",
    "]:\n",
    "    # create environment\n",
    "    e = env.Env()\n",
    "\n",
    "    # randomize task location\n",
    "    e.state.tasks = np.zeros((BOARD_XSIZE, BOARD_YSIZE), dtype=np.bool8)\n",
    "    e.state.tasks[random_valid_location()] = True\n",
    "    e.state.tasks[random_valid_location()] = True\n",
    "\n",
    "\n",
    "    # create actor player at random location\n",
    "    actor_playerstate = env.PlayerState(random_valid_location(), actor_is_impostor, False)\n",
    "    # create other players at random locations\n",
    "    other_playerstate = [env.PlayerState(random_valid_location(), False, False) for _ in other_engines]\n",
    "    # If the actor is not an impostor, then the impostor is randomly chosen from the others.\n",
    "    if not actor_is_impostor:\n",
    "        random.choice(other_playerstate).impostor = True\n",
    "\n",
    "    # set the players in the environment\n",
    "    e.state.players = [actor_playerstate] + other_playerstate\n",
    "    # set the player engines\n",
    "    player_engines = [actor_engine] + other_engines\n",
    "\n",
    "    assert e.state.players[0].impostor == actor_is_impostor\n",
    "\n",
    "    # shuffle the player indices such that the corresponding player states and engines have the same indices\n",
    "    random_indices = np.random.permutation(len(player_engines))\n",
    "    e.state.players = [e.state.players[i] for i in random_indices]\n",
    "    player_engines = [player_engines[i] for i in random_indices]\n",
    "\n",
    "    actor_index = env.Player(np.argwhere(random_indices==0)[0][0])\n",
    "\n",
    "    s_t: list[env.Observation] = []\n",
    "    a_t: list[env.Action] = []\n",
    "    p_t: list[np.ndarray] = []\n",
    "    r_t: list[env.Reward] = []\n",
    "    # play the game\n",
    "    while not e.game_over():\n",
    "        for player, player_engine in enumerate(player_engines):\n",
    "            player = env.Player(player)\n",
    "            # if the player is the actor we're gathering data for, then we need to store the data\n",
    "            if player == actor_index:\n",
    "                obs, action_probs, chosen_action = player_engine.play(player, e)\n",
    "                s_t += [obs]\n",
    "                p_t += [action_probs]\n",
    "                a_t += [chosen_action]\n",
    "                e.play(chosen_action, player)\n",
    "            else:\n",
    "                # skip dead players\n",
    "                if e.game_over_for(player):\n",
    "                    continue\n",
    "                # get chosen action from player engine and play it\n",
    "                _, _, chosen_action = player_engine.play(player, e)\n",
    "                e.play(chosen_action, player)\n",
    "        # step and get rewards\n",
    "        rewards = e.step()\n",
    "        r_t += [rewards[actor_index]]\n",
    "        # if the actor we're gathering data for is dead, then we need to stop\n",
    "        if e.game_over_for(env.Player(actor_index)):\n",
    "            break\n",
    "\n",
    "    # compute advantage and value\n",
    "    critic_network = impostor_critic if actor_is_impostor else crewmate_critic\n",
    "    d_t = network.compute_advantage(critic_network, s_t, r_t)\n",
    "    v_t = network.compute_value(r_t)\n",
    "\n",
    "    return s_t, a_t, p_t, r_t, d_t, v_t, actor_is_impostor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Entropy is too low!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 47\u001b[0m\n\u001b[1;32m     43\u001b[0m     futures\u001b[39m.\u001b[39mappend(future)\n\u001b[1;32m     46\u001b[0m \u001b[39mfor\u001b[39;00m future \u001b[39min\u001b[39;00m concurrent\u001b[39m.\u001b[39mfutures\u001b[39m.\u001b[39mas_completed(futures):\n\u001b[0;32m---> 47\u001b[0m     s_t, a_t, p_t, r_t, d_t, v_t, was_impostor \u001b[39m=\u001b[39m future\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m     49\u001b[0m     \u001b[39mif\u001b[39;00m was_impostor:\n\u001b[1;32m     50\u001b[0m         impostor_s_batch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m s_t\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    453\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs)\n\u001b[1;32m     59\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfuture\u001b[39m.\u001b[39mset_exception(exc)\n",
      "Cell \u001b[0;32mIn[107], line 67\u001b[0m, in \u001b[0;36mplay\u001b[0;34m(actor_engine, actor_is_impostor, other_engines)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m     66\u001b[0m         \u001b[39m# get chosen action from player engine and play it\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m         _, _, chosen_action \u001b[39m=\u001b[39m player_engine\u001b[39m.\u001b[39;49mplay(player, e)\n\u001b[1;32m     68\u001b[0m         e\u001b[39m.\u001b[39mplay(chosen_action, player)\n\u001b[1;32m     69\u001b[0m \u001b[39m# step and get rewards\u001b[39;00m\n",
      "File \u001b[0;32m~/myworkspace/omegasus/amogus-rl/player.py:55\u001b[0m, in \u001b[0;36mActorPlayer.play\u001b[0;34m(self, player, e)\u001b[0m\n\u001b[1;32m     52\u001b[0m action_entropy \u001b[39m=\u001b[39m scipy\u001b[39m.\u001b[39mstats\u001b[39m.\u001b[39mentropy(action_probs)\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m action_entropy \u001b[39m<\u001b[39m \u001b[39m0.001\u001b[39m:\n\u001b[0;32m---> 55\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mEntropy is too low!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m legal_mask \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39mlegal_mask(player)\n\u001b[1;32m     59\u001b[0m raw_p \u001b[39m=\u001b[39m action_probs\u001b[39m*\u001b[39mlegal_mask\n",
      "\u001b[0;31mValueError\u001b[0m: Entropy is too low!"
     ]
    }
   ],
   "source": [
    "impostor_reward_buf:list[float] = []\n",
    "crewmate_reward_buf:list[float] = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    for _ in range(TRAIN_EPOCHS):\n",
    "        crewmate_s_batch:list[env.Observation] = []\n",
    "        crewmate_a_batch:list[env.Action] = []\n",
    "        crewmate_p_batch:list[np.ndarray] = []\n",
    "        crewmate_r_batch:list[env.Reward] = []\n",
    "        crewmate_d_batch:list[env.Advantage] = []\n",
    "        crewmate_v_batch:list[env.Value] = []\n",
    "        \n",
    "        impostor_s_batch:list[env.Observation] = []\n",
    "        impostor_a_batch:list[env.Action] = []\n",
    "        impostor_p_batch:list[np.ndarray] = []\n",
    "        impostor_r_batch:list[env.Reward] = []\n",
    "        impostor_d_batch:list[env.Advantage] = []\n",
    "        impostor_v_batch:list[env.Value] = []\n",
    "\n",
    "        # create actor player\n",
    "        nn_player = player.ActorPlayer(\n",
    "            impostor_actor, impostor_critic, impostor_step,\n",
    "            crewmate_actor, crewmate_critic, crewmate_step,\n",
    "        )\n",
    "\n",
    "\n",
    "        futures = []\n",
    "        for i in range(EPISODES_PER_AGENT):\n",
    "            others = []\n",
    "            for _ in range(3):\n",
    "                others.append(nn_player)\n",
    "            # play the game\n",
    "            future = executor.submit(play, nn_player, False, others)\n",
    "            futures.append(future)\n",
    "\n",
    "        for i in range(EPISODES_PER_AGENT):\n",
    "            others = []\n",
    "            for _ in range(3):\n",
    "                others.append(nn_player)\n",
    "\n",
    "            # play the game\n",
    "            future = executor.submit(play, nn_player, True, others)\n",
    "            futures.append(future)\n",
    "\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            s_t, a_t, p_t, r_t, d_t, v_t, was_impostor = future.result()\n",
    "\n",
    "            if was_impostor:\n",
    "                impostor_s_batch += s_t\n",
    "                impostor_a_batch += a_t\n",
    "                impostor_p_batch += p_t\n",
    "                impostor_r_batch += r_t\n",
    "                impostor_d_batch += d_t\n",
    "                impostor_v_batch += v_t\n",
    "\n",
    "                # statistics\n",
    "                r = np.sum(r_t)\n",
    "                impostor_reward_buf.append(r)\n",
    "                if r < 0:\n",
    "                    raise Exception(\"negative reward\")\n",
    "            else:\n",
    "                crewmate_s_batch += s_t\n",
    "                crewmate_a_batch += a_t\n",
    "                crewmate_p_batch += p_t\n",
    "                crewmate_r_batch += r_t\n",
    "                crewmate_d_batch += d_t\n",
    "                crewmate_v_batch += v_t\n",
    "    \n",
    "                # statistics\n",
    "                r = np.sum(r_t)\n",
    "                crewmate_reward_buf.append(r)\n",
    "\n",
    "\n",
    "        crewmate_actor_losses, crewmate_critic_losses = network.train_ppo(\n",
    "            crewmate_actor,\n",
    "            crewmate_critic,\n",
    "            crewmate_actor_optimizer,\n",
    "            crewmate_critic_optimizer,\n",
    "            crewmate_s_batch,\n",
    "            crewmate_a_batch,\n",
    "            crewmate_p_batch,\n",
    "            crewmate_d_batch,\n",
    "            crewmate_v_batch\n",
    "        )\n",
    "\n",
    "        impostor_actor_losses, impostor_critic_losses = network.train_ppo(\n",
    "            impostor_actor,\n",
    "            impostor_critic,\n",
    "            impostor_actor_optimizer,\n",
    "            impostor_critic_optimizer,\n",
    "            impostor_s_batch,\n",
    "            impostor_a_batch,\n",
    "            impostor_p_batch,\n",
    "            impostor_d_batch,\n",
    "            impostor_v_batch\n",
    "        )\n",
    "\n",
    "        for crewmate_actor_loss, crewmate_critic_loss, impostor_actor_loss, impostor_critic_loss in zip(crewmate_actor_losses, crewmate_critic_losses, impostor_actor_losses, impostor_critic_losses):\n",
    "            writer.add_scalar('impostor_actor_loss', impostor_actor_loss, impostor_step)\n",
    "            writer.add_scalar('impostor_critic_loss', impostor_critic_loss, impostor_step)\n",
    "\n",
    "            writer.add_scalar('crewmate_actor_loss', crewmate_actor_loss, crewmate_step)\n",
    "            writer.add_scalar('crewmate_critic_loss', crewmate_critic_loss, crewmate_step)\n",
    "\n",
    "            if impostor_step % SUMMARY_STATS_INTERVAL == 0:\n",
    "                writer.add_scalar('impostor_reward', np.mean(impostor_reward_buf), impostor_step)\n",
    "                impostor_reward_buf = []\n",
    "            \n",
    "            if crewmate_step % SUMMARY_STATS_INTERVAL == 0:\n",
    "                writer.add_scalar('crewmate_reward', np.mean(crewmate_reward_buf), crewmate_step)\n",
    "                crewmate_reward_buf = []\n",
    "\n",
    "            # Save the neural net parameters to disk.\n",
    "            if impostor_step % MODEL_SAVE_INTERVAL == 0:\n",
    "                torch.save(impostor_actor.state_dict(), f\"{SUMMARY_DIR}/impostor_model_ep_{impostor_step}_actor.ckpt\")\n",
    "                torch.save(impostor_critic.state_dict(), f\"{SUMMARY_DIR}/impostor_model_ep_{impostor_step}_critic.ckpt\")\n",
    "\n",
    "            # Save the neural net parameters to disk.\n",
    "            if crewmate_step % MODEL_SAVE_INTERVAL == 0:\n",
    "                torch.save(crewmate_actor.state_dict(), f\"{SUMMARY_DIR}/crewmate_model_ep_{crewmate_step}_actor.ckpt\")\n",
    "                torch.save(crewmate_critic.state_dict(), f\"{SUMMARY_DIR}/crewmate_model_ep_{crewmate_step}_critic.ckpt\")\n",
    "            \n",
    "            crewmate_step += 1\n",
    "            impostor_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "others = []\n",
    "for _ in range(3):\n",
    "    others.append(nn_player)\n",
    "# play the game            \n",
    "s_t, a_t, p_t, r_t, d_t, v_t, was_impostor = play(nn_player, False, others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "⬛⬛⬛⬛👽\n",
      "⬛⬛😇⬛⬛\n",
      "⬛⬛⬛⬛⬛\n",
      "⬛⬛⬛⬛🧑‍🚀\n",
      "📦⬛⬛⬛🧑‍🚀\n",
      "Move Right\n",
      "probs: [4.3988047e-04 9.9840742e-01 4.1596001e-04 3.8363144e-04 3.5310906e-04]\n",
      "reward: 0.0\n",
      "advantage: -1.5067912803123955\n",
      "value: -1.4348907000000004\n",
      "-----------------------------------\n",
      "⬛⬛⬛⬛⬛\n",
      "⬛⬛📦😇👽\n",
      "⬛⬛⬛⬛⬛\n",
      "⬛⬛⬛⬛⬛\n",
      "📦⬛⬛⬛🧑‍🚀\n",
      "Move Right\n",
      "probs: [3.4275666e-04 9.9855870e-01 5.8528554e-04 2.7464225e-04 2.3862197e-04]\n",
      "reward: 0.0\n",
      "advantage: -1.6742125336804394\n",
      "value: -1.5943230000000004\n",
      "-----------------------------------\n",
      "⬛⬛⬛⬛⬛\n",
      "⬛⬛📦⬛😇\n",
      "⬛⬛⬛⬛👽\n",
      "⬛⬛⬛⬛⬛\n",
      "📦⬛⬛🧑‍🚀⬛\n",
      "Move Up\n",
      "probs: [2.8855400e-04 9.9874264e-01 3.7962836e-04 3.4216474e-04 2.4693998e-04]\n",
      "reward: 0.0\n",
      "advantage: -1.8602361485338215\n",
      "value: -1.7714700000000003\n",
      "-----------------------------------\n",
      "⬛⬛⬛⬛😇\n",
      "⬛⬛📦⬛⬛\n",
      "⬛⬛⬛👽⬛\n",
      "⬛⬛⬛⬛⬛\n",
      "📦⬛⬛⬛🧑‍🚀\n",
      "Wait\n",
      "probs: [3.8578353e-04 9.9830973e-01 2.3813629e-04 6.8116147e-04 3.8509542e-04]\n",
      "reward: 0.0\n",
      "advantage: -2.066929053926468\n",
      "value: -1.9683000000000004\n",
      "-----------------------------------\n",
      "⬛⬛⬛⬛😇\n",
      "⬛⬛📦👽⬛\n",
      "⬛⬛⬛⬛⬛\n",
      "⬛⬛⬛⬛🧑‍🚀\n",
      "📦⬛⬛⬛🧑‍🚀\n",
      "Move Down\n",
      "probs: [8.23975279e-05 9.99584615e-01 5.96210120e-05 1.59612246e-04\n",
      " 1.13927956e-04]\n",
      "reward: 0.0\n",
      "advantage: -2.2965878376960758\n",
      "value: -2.1870000000000003\n",
      "-----------------------------------\n",
      "⬛⬛⬛⬛⬛\n",
      "⬛⬛📦👽😇\n",
      "⬛⬛⬛⬛⬛\n",
      "⬛⬛⬛⬛⬛\n",
      "📦⬛⬛🧑‍🚀🧑‍🚀\n",
      "Move Down\n",
      "probs: [1.3692900e-04 9.9935883e-01 1.6210956e-04 1.8917782e-04 1.5301646e-04]\n",
      "reward: 0.0\n",
      "advantage: -2.5517642641067506\n",
      "value: -2.43\n",
      "-----------------------------------\n",
      "⬛⬛⬛⬛⬛\n",
      "⬛⬛📦⬛👽\n",
      "⬛⬛⬛⬛😇\n",
      "⬛⬛⬛⬛⬛\n",
      "📦⬛⬛⬛🧑‍🚀\n",
      "Move Up\n",
      "probs: [9.1952490e-05 9.9947327e-01 9.9347671e-05 2.2356701e-04 1.1185896e-04]\n",
      "reward: 0.0\n",
      "advantage: -2.8352936267852784\n",
      "value: -2.7\n",
      "-----------------------------------\n",
      "⬛⬛⬛⬛⬛\n",
      "⬛⬛📦👽😇\n",
      "⬛⬛⬛⬛⬛\n",
      "⬛⬛⬛⬛🧑‍🚀\n",
      "📦⬛⬛⬛🧑‍🚀\n",
      "Wait\n",
      "probs: [4.797775e-05 9.997154e-01 7.333379e-05 9.348001e-05 6.979570e-05]\n",
      "reward: -3.0\n",
      "advantage: -3.1503262519836426\n",
      "value: -3.0\n"
     ]
    }
   ],
   "source": [
    "for s, a, p, r, d, v in zip(s_t, a_t, p_t, r_t, d_t, v_t):\n",
    "    print(\"-----------------------------------\")\n",
    "    env.print_obs(s)\n",
    "    env.print_action(a)\n",
    "    print(\"probs:\", p)\n",
    "    print(\"reward:\", r)\n",
    "    print(\"advantage:\", d)\n",
    "    print(\"value:\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s, a, p, r, d, v in zip(crewmate_s_batch, crewmate_a_batch, crewmate_p_batch, crewmate_r_batch, crewmate_d_batch, crewmate_v_batch):\n",
    "    print(\"-----------------------------------\")\n",
    "    env.print_obs(s)\n",
    "    env.print_action(a)\n",
    "    print(p)\n",
    "    print(r)\n",
    "    print(d)\n",
    "    print(v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
