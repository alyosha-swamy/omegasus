{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "import concurrent.futures\n",
    "from torch import optim\n",
    "import torch\n",
    "import os\n",
    "import copy\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import env\n",
    "import network\n",
    "import player\n",
    "\n",
    "\n",
    "BOARD_XSIZE = env.BOARD_XSIZE\n",
    "BOARD_YSIZE = env.BOARD_YSIZE\n",
    "\n",
    "DIMS=(BOARD_XSIZE,BOARD_YSIZE)\n",
    "\n",
    "\n",
    "EPISODES_PER_AGENT = 100\n",
    "TRAIN_EPOCHS = 500000\n",
    "MODEL_SAVE_INTERVAL = 100\n",
    "MAKE_OPPONENT_INTERVAL = 1000\n",
    "SUMMARY_STATS_INTERVAL = 10\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "SUMMARY_DIR = './summary'\n",
    "MODEL_DIR = './models'\n",
    "\n",
    "# create result directory\n",
    "if not os.path.exists(SUMMARY_DIR):\n",
    "    os.makedirs(SUMMARY_DIR)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "cuda = torch.device(\"cuda\")\n",
    "cpu = torch.device(\"cpu\")\n",
    "\n",
    "if use_cuda:\n",
    "    device = cuda\n",
    "else:\n",
    "    device = cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 00:56:29.607022: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-16 00:56:29.935301: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-16 00:56:30.881474: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-16 00:56:30.881754: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-16 00:56:30.881769: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# TODO: restore neural net parameters\n",
    "\n",
    "impostor_actor = network.Actor().to(device)\n",
    "impostor_critic = network.Critic().to(device)\n",
    "impostor_actor_optimizer = optim.Adam(impostor_actor.parameters(), lr=network.ACTOR_LR)\n",
    "impostor_critic_optimizer = optim.Adam(impostor_critic.parameters(), lr=network.CRITIC_LR)\n",
    "\n",
    "crewmate_actor = network.Actor().to(device)\n",
    "crewmate_critic = network.Critic().to(device)\n",
    "crewmate_actor_optimizer = optim.Adam(crewmate_actor.parameters(), lr=network.ACTOR_LR)\n",
    "crewmate_critic_optimizer = optim.Adam(crewmate_critic.parameters(), lr=network.CRITIC_LR)\n",
    "\n",
    "# Get Writer\n",
    "writer = SummaryWriter(log_dir=SUMMARY_DIR)\n",
    "\n",
    "step=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'env' has no attribute 'PLAYER2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m entropy_buf:\u001b[39mlist\u001b[39m[\u001b[39mfloat\u001b[39m] \u001b[39m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m opponent_pool:\u001b[39mlist\u001b[39m[player\u001b[39m.\u001b[39mPlayer] \u001b[39m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m     \u001b[39m#player.RandomPlayer(env.PLAYER2),\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     player\u001b[39m.\u001b[39mRandomPlayer(env\u001b[39m.\u001b[39;49mPLAYER2, \u001b[39m2\u001b[39m),\n\u001b[1;32m      6\u001b[0m ]\n\u001b[1;32m      8\u001b[0m rewards_vs: \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, \u001b[39mlist\u001b[39m[\u001b[39mfloat\u001b[39m]] \u001b[39m=\u001b[39m {}\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'env' has no attribute 'PLAYER2'"
     ]
    }
   ],
   "source": [
    "entropy_buf:list[float] = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_valid_location() -> tuple[int, int]:\n",
    "    x = np.random.randint(0, BOARD_XSIZE)\n",
    "    y = np.random.randint(0, BOARD_YSIZE)\n",
    "    return (x, y)\n",
    "\n",
    "\n",
    "def play(actor: player.ActorPlayer, actor_is_impostor: bool, others: list[player.Player]) -> tuple[\n",
    "    list[env.Observation],\n",
    "    list[env.Action],\n",
    "    list[np.ndarray],\n",
    "    list[env.Reward],\n",
    "    list[env.Advantage],\n",
    "    list[env.Reward],\n",
    "]:\n",
    "    e = env.Env()\n",
    "\n",
    "    # create the players at random locations on the board.\n",
    "    e.state.players = [env.PlayerState(\n",
    "        random_valid_location(), actor_is_impostor, False)]\n",
    "    e.state.players += [env.PlayerState(random_valid_location(), False, True)\n",
    "                        for _ in others]\n",
    "    # If the actor is not an impostor, then the impostor is randomly chosen from the others.\n",
    "    if not actor_is_impostor:\n",
    "        e.state.players[np.random.randint(\n",
    "            1, len(e.state.players))].impostor = True\n",
    "\n",
    "    players = [actor] + others\n",
    "\n",
    "    s_t: list[env.Observation] = []\n",
    "    a_t: list[env.Action] = []\n",
    "    p_t: list[np.ndarray] = []\n",
    "    r_t: list[env.Reward] = []\n",
    "    # play the game\n",
    "    while not e.game_over():\n",
    "        for playerid, player in enumerate(players):\n",
    "            if player == actor:\n",
    "                obs, action_probs, chosen_action, reward = actor.play(env.Player(playerid), e)\n",
    "                s_t += [obs]\n",
    "                p_t += [action_probs]\n",
    "                a_t += [chosen_action]\n",
    "                r_t += [reward]\n",
    "            else:\n",
    "                player.play(env.Player(playerid), e)\n",
    "\n",
    "    # compute advantage and value\n",
    "    d_t = network.compute_advantage(actor.critic, s_t, r_t)\n",
    "    v_t = network.compute_value(r_t)\n",
    "\n",
    "    return s_t, a_t, p_t, r_t, d_t, v_t\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
