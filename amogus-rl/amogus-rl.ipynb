{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from dataclasses import dataclass\n",
    "import concurrent.futures\n",
    "from collections import defaultdict\n",
    "import typing\n",
    "from torch import optim\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import env\n",
    "import network\n",
    "import player\n",
    "\n",
    "\n",
    "BOARD_XSIZE = env.BOARD_XSIZE\n",
    "BOARD_YSIZE = env.BOARD_YSIZE\n",
    "\n",
    "DIMS=(BOARD_XSIZE,BOARD_YSIZE)\n",
    "\n",
    "\n",
    "EPISODES_PER_AGENT = 32\n",
    "TRAIN_EPOCHS = 500000\n",
    "MODEL_SAVE_INTERVAL = 100\n",
    "MAKE_OPPONENT_INTERVAL = 2000\n",
    "SUMMARY_STATS_INTERVAL = 10\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "SUMMARY_DIR = './summary'\n",
    "MODEL_DIR = './models'\n",
    "\n",
    "# create result directory\n",
    "if not os.path.exists(SUMMARY_DIR):\n",
    "    os.makedirs(SUMMARY_DIR)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "cuda = torch.device(\"cuda\")\n",
    "cpu = torch.device(\"cpu\")\n",
    "\n",
    "if use_cuda:\n",
    "    device = cuda\n",
    "else:\n",
    "    device = cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: restore neural net parameters\n",
    "\n",
    "impostor_actor = network.Actor().to(device)\n",
    "impostor_critic = network.Critic().to(device)\n",
    "impostor_actor_optimizer = optim.Adam(impostor_actor.parameters(), lr=network.ACTOR_LR)\n",
    "impostor_critic_optimizer = optim.Adam(impostor_critic.parameters(), lr=network.CRITIC_LR)\n",
    "\n",
    "crewmate_actor = network.Actor().to(device)\n",
    "crewmate_critic = network.Critic().to(device)\n",
    "crewmate_actor_optimizer = optim.Adam(crewmate_actor.parameters(), lr=network.ACTOR_LR)\n",
    "crewmate_critic_optimizer = optim.Adam(crewmate_critic.parameters(), lr=network.CRITIC_LR)\n",
    "\n",
    "# Get Writer\n",
    "writer = SummaryWriter(log_dir=SUMMARY_DIR)\n",
    "\n",
    "impostor_step = 0\n",
    "crewmate_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_valid_location() -> tuple[int, int]:\n",
    "    x = np.random.randint(0, BOARD_XSIZE)\n",
    "    y = np.random.randint(0, BOARD_YSIZE)\n",
    "    return (x, y)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GameSummary:\n",
    "    actor_index: int \n",
    "    engines: list[player.Player]\n",
    "    is_impostor: list[bool]\n",
    "\n",
    "    def configuration(self) -> str:\n",
    "        names = sorted(e.name() for i,e in enumerate(self.engines) if i != self.actor_index)\n",
    "        return f\"{' '.join(names)}\"\n",
    "\n",
    "def play(actor_engine: player.ActorPlayer, actor_is_impostor: bool, other_engines: list[player.Player]) -> tuple[\n",
    "    list[env.Observation],\n",
    "    list[env.Action],\n",
    "    list[np.ndarray],\n",
    "    list[env.Reward],\n",
    "    list[env.Advantage],\n",
    "    list[env.Reward],\n",
    "    GameSummary\n",
    "]:\n",
    "    # create environment\n",
    "    initial_state = env.State(\n",
    "        [],\n",
    "        np.zeros((BOARD_XSIZE, BOARD_YSIZE), dtype=np.int8),\n",
    "    )\n",
    "\n",
    "    # randomize task location\n",
    "    for _ in range(5):\n",
    "        location = random_valid_location()\n",
    "        initial_state.tasks[location] += 5\n",
    "\n",
    "    # create actor player at random location\n",
    "    actor_playerstate = env.PlayerState(random_valid_location(), actor_is_impostor, False)\n",
    "    # create other players at random locations\n",
    "    other_playerstate = [env.PlayerState(random_valid_location(), False, False) for _ in other_engines]\n",
    "    # If the actor is not an impostor, then the impostor is randomly chosen from the others.\n",
    "    if not actor_is_impostor:\n",
    "        random.choice(other_playerstate).impostor = True\n",
    "\n",
    "    # set the players in the environment\n",
    "    initial_state.players = [actor_playerstate] + other_playerstate\n",
    "    # set the player engines\n",
    "    player_engines = [actor_engine] + other_engines\n",
    "\n",
    "    # shuffle the player indices such that the corresponding player states and engines have the same indices\n",
    "    random_indices = np.random.permutation(len(player_engines))\n",
    "    initial_state.players = [initial_state.players[i] for i in random_indices]\n",
    "    player_engines = [player_engines[i] for i in random_indices]\n",
    "\n",
    "    actor_index = env.Player(np.argwhere(random_indices==0)[0][0])\n",
    "\n",
    "    e = env.Env(initial_state)\n",
    "\n",
    "    s_t: list[env.Observation] = []\n",
    "    a_t: list[env.Action] = []\n",
    "    p_t: list[np.ndarray] = []\n",
    "    r_t: list[env.Reward] = []\n",
    "    # play the game\n",
    "    while not e.game_over():\n",
    "        for player, player_engine in enumerate(player_engines):\n",
    "            player = env.Player(player)\n",
    "            # if the player is the actor we're gathering data for, then we need to store the data\n",
    "            if player == actor_index:\n",
    "                obs, action_probs, chosen_action = player_engine.play(player, e)\n",
    "                s_t += [obs]\n",
    "                p_t += [action_probs]\n",
    "                a_t += [chosen_action]\n",
    "                e.play(chosen_action, player)\n",
    "            else:\n",
    "                # skip dead players\n",
    "                if e.game_over_for(player):\n",
    "                    continue\n",
    "                # get chosen action from player engine and play it\n",
    "                _, _, chosen_action = player_engine.play(player, e)\n",
    "                e.play(chosen_action, player)\n",
    "        # step and get rewards\n",
    "        rewards = e.step()\n",
    "        r_t += [rewards[actor_index]]\n",
    "        # if the actor we're gathering data for is dead, then we need to stop\n",
    "        if e.game_over_for(env.Player(actor_index)):\n",
    "            break\n",
    "\n",
    "    # compute advantage and value\n",
    "    critic_network = impostor_critic if actor_is_impostor else crewmate_critic\n",
    "    d_t = network.compute_advantage(critic_network, s_t, r_t)\n",
    "    v_t = network.compute_value(r_t)\n",
    "\n",
    "    summary = GameSummary(int(actor_index), player_engines, [e.state.players[i].impostor for i in range(len(e.state.players))])\n",
    "\n",
    "    return s_t, a_t, p_t, r_t, d_t, v_t, summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "opponent_pool: list[player.Player] = [\n",
    "    player.RandomPlayer(),\n",
    "    player.GreedyPlayer(),\n",
    "]\n",
    "\n",
    "# a temp buffer of the current model's rewards\n",
    "crewmate_rewards_vs:defaultdict[tuple[str, str, str], list[float]] = defaultdict(lambda:[])\n",
    "impostor_rewards_vs:defaultdict[tuple[str, str, str], list[float]] = defaultdict(lambda:[])\n",
    "\n",
    "\n",
    "# the history of rewards over time\n",
    "crewmate_rewards_history:defaultdict[tuple[str, str, str], list[tuple[int, float]]] = defaultdict(lambda:[])\n",
    "impostor_rewards_history:defaultdict[tuple[str, str, str], list[tuple[int, float]]] = defaultdict(lambda:[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[161], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m     future \u001b[39m=\u001b[39m executor\u001b[39m.\u001b[39msubmit(play, nn_player, \u001b[39mTrue\u001b[39;00m, others)\n\u001b[1;32m     34\u001b[0m     futures\u001b[39m.\u001b[39mappend(future)\n\u001b[0;32m---> 37\u001b[0m \u001b[39mfor\u001b[39;00m future \u001b[39min\u001b[39;00m concurrent\u001b[39m.\u001b[39mfutures\u001b[39m.\u001b[39mas_completed(futures):\n\u001b[1;32m     38\u001b[0m     s_t, a_t, p_t, r_t, d_t, v_t, game_summary \u001b[39m=\u001b[39m future\u001b[39m.\u001b[39mresult()\n\u001b[1;32m     40\u001b[0m     configuration \u001b[39m=\u001b[39m game_summary\u001b[39m.\u001b[39mconfiguration()\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:245\u001b[0m, in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[39mif\u001b[39;00m wait_timeout \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    241\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m (of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m) futures unfinished\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (\n\u001b[1;32m    243\u001b[0m                 \u001b[39mlen\u001b[39m(pending), total_futures))\n\u001b[0;32m--> 245\u001b[0m waiter\u001b[39m.\u001b[39;49mevent\u001b[39m.\u001b[39;49mwait(wait_timeout)\n\u001b[1;32m    247\u001b[0m \u001b[39mwith\u001b[39;00m waiter\u001b[39m.\u001b[39mlock:\n\u001b[1;32m    248\u001b[0m     finished \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39mfinished_futures\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    608\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    for _ in range(TRAIN_EPOCHS):\n",
    "        crewmate_s_batch:list[env.Observation] = []\n",
    "        crewmate_a_batch:list[env.Action] = []\n",
    "        crewmate_p_batch:list[np.ndarray] = []\n",
    "        crewmate_r_batch:list[env.Reward] = []\n",
    "        crewmate_d_batch:list[env.Advantage] = []\n",
    "        crewmate_v_batch:list[env.Value] = []\n",
    "        \n",
    "        impostor_s_batch:list[env.Observation] = []\n",
    "        impostor_a_batch:list[env.Action] = []\n",
    "        impostor_p_batch:list[np.ndarray] = []\n",
    "        impostor_r_batch:list[env.Reward] = []\n",
    "        impostor_d_batch:list[env.Advantage] = []\n",
    "        impostor_v_batch:list[env.Value] = []\n",
    "\n",
    "        # create actor player\n",
    "        nn_player = player.ActorPlayer(\n",
    "            impostor_actor, impostor_critic, impostor_step,\n",
    "            crewmate_actor, crewmate_critic, crewmate_step,\n",
    "        )\n",
    "\n",
    "        futures = []\n",
    "        for i in range(EPISODES_PER_AGENT):\n",
    "            others = [random.choice(opponent_pool)]*3\n",
    "            # play the game\n",
    "            future = executor.submit(play, nn_player, False, others)\n",
    "            futures.append(future)\n",
    "\n",
    "        for i in range(EPISODES_PER_AGENT):\n",
    "            others = [random.choice(opponent_pool)]*3\n",
    "            # play the game\n",
    "            future = executor.submit(play, nn_player, True, others)\n",
    "            futures.append(future)\n",
    "\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            s_t, a_t, p_t, r_t, d_t, v_t, game_summary = future.result()\n",
    "\n",
    "            configuration = game_summary.configuration()\n",
    "            r = np.sum(r_t)\n",
    "\n",
    "            if game_summary.is_impostor[game_summary.actor_index]:\n",
    "                impostor_s_batch += s_t\n",
    "                impostor_a_batch += a_t\n",
    "                impostor_p_batch += p_t\n",
    "                impostor_r_batch += r_t\n",
    "                impostor_d_batch += d_t\n",
    "                impostor_v_batch += v_t\n",
    "                # statistics\n",
    "                impostor_rewards_vs[configuration] += [r]\n",
    "            else:\n",
    "                crewmate_s_batch += s_t\n",
    "                crewmate_a_batch += a_t\n",
    "                crewmate_p_batch += p_t\n",
    "                crewmate_r_batch += r_t\n",
    "                crewmate_d_batch += d_t\n",
    "                crewmate_v_batch += v_t\n",
    "                # statistics\n",
    "                crewmate_rewards_vs[configuration] += [r]\n",
    "\n",
    "\n",
    "        crewmate_actor_losses, crewmate_critic_losses = network.train_ppo(\n",
    "            crewmate_actor,\n",
    "            crewmate_critic,\n",
    "            crewmate_actor_optimizer,\n",
    "            crewmate_critic_optimizer,\n",
    "            crewmate_s_batch,\n",
    "            crewmate_a_batch,\n",
    "            crewmate_p_batch,\n",
    "            crewmate_d_batch,\n",
    "            crewmate_v_batch\n",
    "        )\n",
    "\n",
    "        impostor_actor_losses, impostor_critic_losses = network.train_ppo(\n",
    "            impostor_actor,\n",
    "            impostor_critic,\n",
    "            impostor_actor_optimizer,\n",
    "            impostor_critic_optimizer,\n",
    "            impostor_s_batch,\n",
    "            impostor_a_batch,\n",
    "            impostor_p_batch,\n",
    "            impostor_d_batch,\n",
    "            impostor_v_batch\n",
    "        )\n",
    "\n",
    "        for crewmate_actor_loss, crewmate_critic_loss, impostor_actor_loss, impostor_critic_loss in zip(crewmate_actor_losses, crewmate_critic_losses, impostor_actor_losses, impostor_critic_losses):\n",
    "            writer.add_scalar('impostor_actor_loss', impostor_actor_loss, impostor_step)\n",
    "            writer.add_scalar('impostor_critic_loss', impostor_critic_loss, impostor_step)\n",
    "\n",
    "            writer.add_scalar('crewmate_actor_loss', crewmate_actor_loss, crewmate_step)\n",
    "            writer.add_scalar('crewmate_critic_loss', crewmate_critic_loss, crewmate_step)\n",
    "\n",
    "            if impostor_step % SUMMARY_STATS_INTERVAL == 0:\n",
    "                for opponent_name, rewards in impostor_rewards_vs.items():\n",
    "                    if len(rewards) > 50:\n",
    "                        avg_reward = np.array(rewards).mean()\n",
    "                        writer.add_scalar(f'impostor_reward_against_{opponent_name}', avg_reward, impostor_step)\n",
    "                        impostor_rewards_vs[opponent_name] = []\n",
    "                        impostor_rewards_history[opponent_name] += [(impostor_step, avg_reward)]\n",
    "\n",
    "            if crewmate_step % SUMMARY_STATS_INTERVAL == 0:\n",
    "                for opponent_name, rewards in crewmate_rewards_vs.items():\n",
    "                    if len(rewards) > 50:\n",
    "                        avg_reward = np.array(rewards).mean()\n",
    "                        writer.add_scalar(f'crewmate_reward_against_{opponent_name}', avg_reward, crewmate_step)\n",
    "                        crewmate_rewards_vs[opponent_name] = []\n",
    "                        crewmate_rewards_history[opponent_name] += [(crewmate_step, avg_reward)]\n",
    "\n",
    "            def clone_nn(nn):\n",
    "                new_nn = copy.deepcopy(nn)\n",
    "                new_nn.eval()\n",
    "                new_nn.to(device)\n",
    "                return new_nn\n",
    "\n",
    "            if impostor_step % MAKE_OPPONENT_INTERVAL == 0:\n",
    "                # create a new opponent\n",
    "                frozen_impostor_actor = clone_nn(impostor_actor)\n",
    "                frozen_impostor_critic = clone_nn(impostor_critic)\n",
    "                frozen_crewmate_actor = clone_nn(crewmate_actor)\n",
    "                frozen_crewmate_critic = clone_nn(crewmate_critic)\n",
    "                frozen_nn_player = player.ActorPlayer(\n",
    "                    frozen_impostor_actor, frozen_impostor_critic, impostor_step,\n",
    "                    frozen_crewmate_actor, frozen_crewmate_critic, crewmate_step,\n",
    "                )\n",
    "                opponent_pool.append(frozen_nn_player)\n",
    "\n",
    "            # Save the neural net parameters to disk.\n",
    "            if impostor_step % MODEL_SAVE_INTERVAL == 0:\n",
    "                torch.save(impostor_actor.state_dict(), f\"{SUMMARY_DIR}/impostor_model_ep_{impostor_step}_actor.ckpt\")\n",
    "                torch.save(impostor_critic.state_dict(), f\"{SUMMARY_DIR}/impostor_model_ep_{impostor_step}_critic.ckpt\")\n",
    "\n",
    "            # Save the neural net parameters to disk.\n",
    "            if crewmate_step % MODEL_SAVE_INTERVAL == 0:\n",
    "                torch.save(crewmate_actor.state_dict(), f\"{SUMMARY_DIR}/crewmate_model_ep_{crewmate_step}_actor.ckpt\")\n",
    "                torch.save(crewmate_critic.state_dict(), f\"{SUMMARY_DIR}/crewmate_model_ep_{crewmate_step}_critic.ckpt\")\n",
    "            \n",
    "            crewmate_step += 1\n",
    "            impostor_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "others = []\n",
    "for _ in range(3):\n",
    "    others.append(player.RandomPlayer())\n",
    "# play the game            \n",
    "s_t, a_t, p_t, r_t, d_t, v_t, _ = play(nn_player, False, others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "⬛⬛⬛🧑‍🚀⬛\n",
      "⬛📦⬛📦⬛\n",
      "📦⬛🧑‍🚀⬛📦\n",
      "⬛⬛👽⬛⬛\n",
      "😇⬛⬛⬛📦\n",
      "Move Up\n",
      "probs: [0.01174184 0.5334249  0.26867872 0.00117789 0.18497665]\n",
      "reward: 0.0\n",
      "advantage: 0.2249156352370617\n",
      "value: 0.22077053931000346\n",
      "-----------------------------------\n",
      "⬛⬛⬛⬛⬛\n",
      "⬛📦⬛🧑‍🚀⬛\n",
      "📦⬛⬛⬛📦\n",
      "😇⬛🧑‍🚀👽⬛\n",
      "⬛⬛⬛⬛📦\n",
      "Wait\n",
      "probs: [0.00761672 0.16497579 0.51022977 0.0057961  0.31138164]\n",
      "reward: 0.0\n",
      "advantage: 0.2811445440463271\n",
      "value: 0.2759631741375043\n",
      "-----------------------------------\n",
      "⬛⬛⬛⬛⬛\n",
      "⬛📦⬛📦🧑‍🚀\n",
      "📦⬛⬛👽📦\n",
      "😇⬛⬛⬛⬛\n",
      "⬛⬛🧑‍🚀⬛📦\n",
      "Move Right\n",
      "probs: [0.00621011 0.2984195  0.49319246 0.01577037 0.1864075 ]\n",
      "reward: 0.0\n",
      "advantage: 0.3514306800579089\n",
      "value: 0.34495396767188036\n",
      "-----------------------------------\n",
      "⬛⬛⬛⬛⬛\n",
      "⬛📦⬛👽⬛\n",
      "📦⬛⬛⬛📦\n",
      "⬛😇🧑‍🚀⬛⬛\n",
      "⬛⬛⬛⬛📦\n",
      "Move Right\n",
      "probs: [0.11557229 0.3443359  0.3956277  0.00412685 0.14033727]\n",
      "reward: 0.0\n",
      "advantage: 0.4392883500723861\n",
      "value: 0.4311924595898504\n",
      "-----------------------------------\n",
      "⬛⬛⬛👽⬛\n",
      "⬛📦🧑‍🚀📦⬛\n",
      "📦⬛⬛⬛📦\n",
      "⬛⬛😇🧑‍🚀⬛\n",
      "⬛⬛⬛⬛📦\n",
      "Move Left\n",
      "probs: [0.08693548 0.8179633  0.05319988 0.01085969 0.0310417 ]\n",
      "reward: 0.0\n",
      "advantage: 0.5491104375904826\n",
      "value: 0.538990574487313\n",
      "-----------------------------------\n",
      "⬛⬛⬛⬛👽\n",
      "⬛📦🧑‍🚀📦⬛\n",
      "📦⬛⬛🧑‍🚀📦\n",
      "⬛😇⬛⬛⬛\n",
      "⬛⬛⬛⬛📦\n",
      "Move Right\n",
      "probs: [0.02304815 0.3682541  0.52340215 0.00843374 0.0768618 ]\n",
      "reward: 0.0\n",
      "advantage: 0.6863880469881032\n",
      "value: 0.6737382181091413\n",
      "-----------------------------------\n",
      "⬛⬛⬛⬛👽\n",
      "⬛📦⬛🧑‍🚀⬛\n",
      "📦⬛⬛⬛🧑‍🚀\n",
      "⬛⬛😇⬛⬛\n",
      "⬛⬛⬛⬛📦\n",
      "Move Right\n",
      "probs: [0.12596886 0.72171694 0.06909148 0.00815833 0.0750644 ]\n",
      "reward: 0.0\n",
      "advantage: 0.8579850587351289\n",
      "value: 0.8421727726364265\n",
      "-----------------------------------\n",
      "⬛⬛⬛👽⬛\n",
      "⬛📦⬛📦🧑‍🚀\n",
      "📦⬛⬛⬛📦\n",
      "⬛⬛⬛😇⬛\n",
      "⬛⬛⬛⬛📦\n",
      "Move Right\n",
      "probs: [0.02440979 0.8589702  0.02562264 0.07182892 0.01916844]\n",
      "reward: 0.0\n",
      "advantage: 1.072481323418911\n",
      "value: 1.0527159657955332\n",
      "-----------------------------------\n",
      "⬛⬛⬛⬛🧑‍🚀\n",
      "⬛📦⬛👽⬛\n",
      "📦⬛⬛⬛📦\n",
      "⬛⬛⬛⬛😇\n",
      "⬛⬛⬛⬛📦\n",
      "Wait\n",
      "probs: [0.21638432 0.02470364 0.05925377 0.44068342 0.25897485]\n",
      "reward: 0.0\n",
      "advantage: 1.3406016542736388\n",
      "value: 1.3158949572444163\n",
      "-----------------------------------\n",
      "⬛⬛⬛🧑‍🚀🧑‍🚀\n",
      "⬛📦⬛📦👽\n",
      "📦⬛⬛⬛📦\n",
      "⬛⬛⬛⬛😇\n",
      "⬛⬛⬛⬛📦\n",
      "Move Down\n",
      "probs: [0.04410071 0.00949288 0.01160518 0.84737855 0.08742259]\n",
      "reward: 0.5\n",
      "advantage: 1.6757520678420483\n",
      "value: 1.6448686965555201\n",
      "-----------------------------------\n",
      "⬛⬛⬛🧑‍🚀⬛\n",
      "⬛📦⬛📦👽\n",
      "📦⬛⬛⬛📦\n",
      "⬛⬛⬛⬛⬛\n",
      "⬛⬛⬛⬛😇\n",
      "Wait\n",
      "probs: [0.06989535 0.02600358 0.3050866  0.00615623 0.5928582 ]\n",
      "reward: 0.5\n",
      "advantage: 1.4696900848025605\n",
      "value: 1.4310858706944\n",
      "-----------------------------------\n",
      "⬛⬛🧑‍🚀⬛⬛\n",
      "⬛📦⬛📦💀\n",
      "📦⬛⬛⬛📦\n",
      "⬛⬛⬛⬛⬛\n",
      "⬛⬛⬛⬛😇\n",
      "Move Up\n",
      "probs: [0.08171874 0.02370419 0.19586824 0.00469136 0.6940175 ]\n",
      "reward: 0.0\n",
      "advantage: 1.2121126060032004\n",
      "value: 1.1638573383680002\n",
      "-----------------------------------\n",
      "⬛⬛⬛⬛👽\n",
      "⬛📦🧑‍🚀📦💀\n",
      "📦⬛⬛⬛📦\n",
      "⬛⬛⬛⬛😇\n",
      "⬛⬛⬛⬛📦\n",
      "Move Up\n",
      "probs: [0.15433472 0.02483697 0.10010333 0.5103491  0.2103759 ]\n",
      "reward: 0.5\n",
      "advantage: 1.5151407575040003\n",
      "value: 1.45482167296\n",
      "-----------------------------------\n",
      "⬛⬛⬛⬛👽\n",
      "⬛🧑‍🚀⬛📦💀\n",
      "📦⬛⬛⬛😇\n",
      "⬛⬛⬛⬛⬛\n",
      "⬛⬛⬛⬛📦\n",
      "Move Left\n",
      "probs: [0.3138907  0.01327602 0.03769294 0.15985572 0.4752846 ]\n",
      "reward: 0.0\n",
      "advantage: 1.2689259468800003\n",
      "value: 1.1935270912000002\n",
      "-----------------------------------\n",
      "⬛⬛⬛👽⬛\n",
      "⬛📦⬛📦💀\n",
      "📦🧑‍🚀⬛😇📦\n",
      "⬛⬛⬛⬛⬛\n",
      "⬛⬛⬛⬛📦\n",
      "Move Right\n",
      "probs: [0.05215812 0.67574835 0.19528095 0.01925442 0.05755812]\n",
      "reward: 0.5\n",
      "advantage: 1.5861574336000002\n",
      "value: 1.4919088640000002\n",
      "-----------------------------------\n",
      "⬛⬛👽⬛⬛\n",
      "⬛📦⬛📦💀\n",
      "📦⬛🧑‍🚀⬛😇\n",
      "⬛⬛⬛⬛⬛\n",
      "⬛⬛⬛⬛📦\n",
      "Wait\n",
      "probs: [0.34141865 0.00969147 0.175943   0.06641142 0.4065355 ]\n",
      "reward: 0.5\n",
      "advantage: 1.357696792\n",
      "value: 1.2398860800000002\n",
      "-----------------------------------\n",
      "⬛⬛⬛⬛⬛\n",
      "⬛📦👽📦💀\n",
      "📦⬛⬛🧑‍🚀😇\n",
      "⬛⬛⬛⬛⬛\n",
      "⬛⬛⬛⬛📦\n",
      "Wait\n",
      "probs: [0.25384635 0.01184442 0.05302538 0.18458004 0.49670386]\n",
      "reward: 0.5\n",
      "advantage: 1.0721209900000002\n",
      "value: 0.9248576000000002\n",
      "-----------------------------------\n",
      "⬛⬛👽⬛⬛\n",
      "⬛📦⬛📦💀\n",
      "📦⬛⬛⬛😇\n",
      "⬛⬛⬛⬛⬛\n",
      "⬛⬛⬛⬛📦\n",
      "Move Left\n",
      "probs: [0.55708086 0.01084833 0.24946842 0.08096702 0.10163535]\n",
      "reward: 0.0\n",
      "advantage: 0.7151512375000002\n",
      "value: 0.5310720000000001\n",
      "-----------------------------------\n",
      "⬛⬛⬛👽⬛\n",
      "⬛📦⬛📦💀\n",
      "📦⬛⬛😇⬛\n",
      "⬛⬛⬛⬛🧑‍🚀\n",
      "⬛⬛⬛⬛📦\n",
      "Move Up\n",
      "probs: [0.2765073  0.04048309 0.54569155 0.06503603 0.07228204]\n",
      "reward: 0.5\n",
      "advantage: 0.8939390468750001\n",
      "value: 0.6638400000000001\n",
      "-----------------------------------\n",
      "⬛⬛👽⬛⬛\n",
      "⬛📦⬛😇💀\n",
      "📦⬛⬛⬛⬛\n",
      "⬛⬛⬛⬛🧑‍🚀\n",
      "⬛⬛⬛⬛📦\n",
      "Move Right\n",
      "probs: [0.30379066 0.23672195 0.03734966 0.22921392 0.19292381]\n",
      "reward: 0.0\n",
      "advantage: 0.4924238085937502\n",
      "value: 0.20480000000000007\n",
      "-----------------------------------\n",
      "⬛👽⬛⬛⬛\n",
      "⬛📦⬛⬛💀\n",
      "📦⬛⬛⬛⬛\n",
      "⬛⬛⬛⬛⬛\n",
      "⬛⬛⬛⬛🧑‍🚀\n",
      "Move Down\n",
      "probs: [0.29851547 0.04682476 0.08613625 0.38371608 0.18480739]\n",
      "reward: 0.0\n",
      "advantage: 0.6155297607421877\n",
      "value: 0.25600000000000006\n",
      "-----------------------------------\n",
      "⬛👽⬛⬛⬛\n",
      "⬛📦⬛⬛💀\n",
      "📦⬛⬛⬛😇\n",
      "⬛⬛⬛⬛🧑‍🚀\n",
      "⬛⬛⬛⬛📦\n",
      "Move Down\n",
      "probs: [0.23110875 0.02325376 0.07460475 0.48763996 0.18339276]\n",
      "reward: 0.0\n",
      "advantage: 0.7694122009277345\n",
      "value: 0.32000000000000006\n",
      "-----------------------------------\n",
      "⬛👽⬛⬛⬛\n",
      "⬛📦⬛⬛💀\n",
      "📦⬛⬛⬛⬛\n",
      "⬛⬛⬛🧑‍🚀😇\n",
      "⬛⬛⬛⬛📦\n",
      "Wait\n",
      "probs: [0.0973473  0.00589727 0.00843621 0.8087633  0.07955597]\n",
      "reward: 0.0\n",
      "advantage: 0.9617652511596682\n",
      "value: 0.4\n",
      "-----------------------------------\n",
      "⬛⬛👽⬛⬛\n",
      "⬛📦⬛⬛💀\n",
      "📦⬛⬛⬛⬛\n",
      "⬛⬛⬛⬛😇\n",
      "⬛⬛⬛⬛📦\n",
      "Move Down\n",
      "probs: [0.09040892 0.00962223 0.00725154 0.82745135 0.06526595]\n",
      "reward: 0.5\n",
      "advantage: 1.2022065639495851\n",
      "value: 0.5\n",
      "-----------------------------------\n",
      "⬛👽⬛⬛⬛\n",
      "⬛📦⬛⬛💀\n",
      "📦⬛⬛⬛🧑‍🚀\n",
      "⬛⬛⬛⬛⬛\n",
      "⬛⬛⬛⬛😇\n",
      "Move Left\n",
      "probs: [0.16862845 0.01623271 0.26379967 0.0061395  0.54519975]\n",
      "reward: 0.0\n",
      "advantage: 0.8777582049369812\n",
      "value: 0.0\n"
     ]
    }
   ],
   "source": [
    "for s, a, p, r, d, v in zip(s_t, a_t, p_t, r_t, d_t, v_t):\n",
    "    print(\"-----------------------------------\")\n",
    "    env.print_obs(s)\n",
    "    env.print_action(a)\n",
    "    print(\"probs:\", p)\n",
    "    print(\"reward:\", r)\n",
    "    print(\"advantage:\", d)\n",
    "    print(\"value:\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s, a, p, r, d, v in zip(crewmate_s_batch, crewmate_a_batch, crewmate_p_batch, crewmate_r_batch, crewmate_d_batch, crewmate_v_batch):\n",
    "    print(\"-----------------------------------\")\n",
    "    env.print_obs(s)\n",
    "    env.print_action(a)\n",
    "    print(p)\n",
    "    print(r)\n",
    "    print(d)\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'greedy greedy greedy': [0.0, 0.0, 2.0, 0.5, 3.0, 1.0, 2.0, 3.0, 1.5, 2.5, -0.5, 0.0, -0.5, 2.0, 2.5, -0.5, 1.0, 3.0, 1.5, 2.0, 1.5, 1.5, 0.5, 0.5, 1.0], 'random random random': [2.0, 4.5, 8.5, 6.0, 9.0, 3.0, 4.5, 9.5, 8.0, 3.5, 7.5, 6.5, 0.0, 8.0, 6.0, 4.5, 7.0, 9.0, 7.5, 0.0, 8.5, 6.0, 6.5, 7.5, 5.5, 5.0, 5.5, 5.5, 3.0, 4.0, 1.0, 6.0], 'nn_ckpt_crewmate0_impostor0 nn_ckpt_crewmate0_impostor0 nn_ckpt_crewmate0_impostor0': [0.0, 1.5, 8.5, 0.0, 9.5, -0.5, 5.5, 4.5, 6.0, 6.0, 1.5, 3.5, 5.5, 6.0, 7.0, 4.5, 7.0, 6.5, 1.0, 6.0, 6.5, 6.0, 0.5, 0.5, 2.5, 8.0, 0.0, 2.0, 8.5, 7.5, 7.5, 7.5, 5.0, 4.5, 7.0, 8.0, 0.0, 5.0, 5.0, 6.5, 6.5, 6.0, 2.0, 3.0, 0.5, 2.0, 7.5, 4.5, 6.0, 6.5], 'nn_ckpt_crewmate2000_impostor2000 nn_ckpt_crewmate2000_impostor2000 nn_ckpt_crewmate2000_impostor2000': [2.0, 8.5, 6.5, 5.5, 0.0, 6.5, 2.5, 7.0, 0.5, 6.0, 6.5, 5.5, 5.0, 1.5, 1.0, 7.5], 'nn_ckpt_crewmate4000_impostor4000 nn_ckpt_crewmate4000_impostor4000 nn_ckpt_crewmate4000_impostor4000': [6.5, 5.0, 4.5, 4.0, 6.5, 3.5, 4.0, 2.0, 5.5, -0.5, 3.5, 4.0, 5.0, 4.0, 1.5, 3.5, 3.0, 2.5, 6.5, 1.5, 4.5, 3.0], 'nn_ckpt_crewmate6000_impostor6000 nn_ckpt_crewmate6000_impostor6000 nn_ckpt_crewmate6000_impostor6000': [7.0, 4.5, 7.0, 0.0, 9.5, 4.0, 2.0, 3.5, 8.0, 8.0, 3.0, 1.5, 1.5, 2.5, 5.5, 5.5, 3.0, 5.0, 3.5, 4.0, 0.0, 3.0, 5.0, -0.5, 2.5, 4.5, 3.5, 3.5, -0.5, 6.0, 8.5, 5.0, 5.0, 4.5, 6.0, 2.0, 3.5, 3.5, 1.0, 8.0, 4.5, 6.5, 7.5, 3.0, 4.5, 6.0, 1.0, 0.0, 3.5, -0.5, 1.5, 4.0, 1.0, 3.0, 4.5], 'nn_ckpt_crewmate8000_impostor8000 nn_ckpt_crewmate8000_impostor8000 nn_ckpt_crewmate8000_impostor8000': [0.0, 3.5, 1.0, 4.5, 6.5, 2.5, 5.0, 1.0, 4.0, 6.5, 0.0, 3.5, 3.0, 5.0, 2.0, 2.5], 'nn_ckpt_crewmate10000_impostor10000 nn_ckpt_crewmate10000_impostor10000 nn_ckpt_crewmate10000_impostor10000': [0.5, 3.5, 8.0, 1.0, 0.5], 'nn_ckpt_crewmate12000_impostor12000 nn_ckpt_crewmate12000_impostor12000 nn_ckpt_crewmate12000_impostor12000': [1.0], 'nn_ckpt_crewmate14000_impostor14000 nn_ckpt_crewmate14000_impostor14000 nn_ckpt_crewmate14000_impostor14000': [4.0, 0.0, 1.5, 2.0, -0.5, 2.0, 3.5, 5.0, 0.5, 3.0, 6.0, 5.5, 1.5, 2.5, 0.5, 3.0, 1.5, -0.5, 5.5, 3.0, -0.5, 2.5, 1.5, 1.0, 2.5, -0.5, 5.0, 2.5, 2.0, 3.5, 5.5, 2.0, -0.5, 0.5, 4.0, 2.0, 0.5, 3.5]}\n"
     ]
    }
   ],
   "source": [
    "print(crewmate_rewards_vs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
