{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "import concurrent.futures\n",
    "from torch import optim\n",
    "import torch\n",
    "import os\n",
    "import copy\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import env\n",
    "import network\n",
    "import player\n",
    "\n",
    "\n",
    "BOARD_XSIZE = env.BOARD_XSIZE\n",
    "BOARD_YSIZE = env.BOARD_YSIZE\n",
    "\n",
    "DIMS=(BOARD_XSIZE,BOARD_YSIZE)\n",
    "\n",
    "\n",
    "EPISODES_PER_AGENT = 100\n",
    "TRAIN_EPOCHS = 500000\n",
    "MODEL_SAVE_INTERVAL = 100\n",
    "MAKE_OPPONENT_INTERVAL = 1000\n",
    "SUMMARY_STATS_INTERVAL = 10\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "SUMMARY_DIR = './summary'\n",
    "MODEL_DIR = './models'\n",
    "\n",
    "# create result directory\n",
    "if not os.path.exists(SUMMARY_DIR):\n",
    "    os.makedirs(SUMMARY_DIR)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "cuda = torch.device(\"cuda\")\n",
    "cpu = torch.device(\"cpu\")\n",
    "\n",
    "if use_cuda:\n",
    "    device = cuda\n",
    "else:\n",
    "    device = cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 00:56:29.607022: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-16 00:56:29.935301: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-16 00:56:30.881474: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-16 00:56:30.881754: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-16 00:56:30.881769: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# TODO: restore neural net parameters\n",
    "\n",
    "impostor_actor = network.Actor().to(device)\n",
    "impostor_critic = network.Critic().to(device)\n",
    "impostor_actor_optimizer = optim.Adam(impostor_actor.parameters(), lr=network.ACTOR_LR)\n",
    "impostor_critic_optimizer = optim.Adam(impostor_critic.parameters(), lr=network.CRITIC_LR)\n",
    "\n",
    "crewmate_actor = network.Actor().to(device)\n",
    "crewmate_critic = network.Critic().to(device)\n",
    "crewmate_actor_optimizer = optim.Adam(crewmate_actor.parameters(), lr=network.ACTOR_LR)\n",
    "crewmate_critic_optimizer = optim.Adam(crewmate_critic.parameters(), lr=network.CRITIC_LR)\n",
    "\n",
    "# Get Writer\n",
    "writer = SummaryWriter(log_dir=SUMMARY_DIR)\n",
    "\n",
    "impostor_step = 0\n",
    "crewmate_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'env' has no attribute 'PLAYER2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m entropy_buf:\u001b[39mlist\u001b[39m[\u001b[39mfloat\u001b[39m] \u001b[39m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m opponent_pool:\u001b[39mlist\u001b[39m[player\u001b[39m.\u001b[39mPlayer] \u001b[39m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m     \u001b[39m#player.RandomPlayer(env.PLAYER2),\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     player\u001b[39m.\u001b[39mRandomPlayer(env\u001b[39m.\u001b[39;49mPLAYER2, \u001b[39m2\u001b[39m),\n\u001b[1;32m      6\u001b[0m ]\n\u001b[1;32m      8\u001b[0m rewards_vs: \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, \u001b[39mlist\u001b[39m[\u001b[39mfloat\u001b[39m]] \u001b[39m=\u001b[39m {}\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'env' has no attribute 'PLAYER2'"
     ]
    }
   ],
   "source": [
    "crewmate_reward_buf:list[float] = []\n",
    "impostor_reward_buf:list[float] = []\n",
    "\n",
    "crewmate_pool : list[player.Player] = [\n",
    "    player.RandomPlayer(),\n",
    "]\n",
    "impostor_pool : list[player.Player] = [\n",
    "    player.RandomPlayer(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_valid_location() -> tuple[int, int]:\n",
    "    x = np.random.randint(0, BOARD_XSIZE)\n",
    "    y = np.random.randint(0, BOARD_YSIZE)\n",
    "    return (x, y)\n",
    "\n",
    "\n",
    "def play(actor: player.ActorPlayer, actor_is_impostor: bool, others: list[player.Player]) -> tuple[\n",
    "    list[env.Observation],\n",
    "    list[env.Action],\n",
    "    list[np.ndarray],\n",
    "    list[env.Reward],\n",
    "    list[env.Advantage],\n",
    "    list[env.Reward],\n",
    "    bool\n",
    "]:\n",
    "    e = env.Env()\n",
    "\n",
    "    # create the players at random locations on the board.\n",
    "    e.state.players = [env.PlayerState(\n",
    "        random_valid_location(), actor_is_impostor, False)]\n",
    "    e.state.players += [env.PlayerState(random_valid_location(), False, True)\n",
    "                        for _ in others]\n",
    "    # If the actor is not an impostor, then the impostor is randomly chosen from the others.\n",
    "    if not actor_is_impostor:\n",
    "        e.state.players[np.random.randint(\n",
    "            1, len(e.state.players))].impostor = True\n",
    "\n",
    "    players = [actor] + others\n",
    "\n",
    "    s_t: list[env.Observation] = []\n",
    "    a_t: list[env.Action] = []\n",
    "    p_t: list[np.ndarray] = []\n",
    "    r_t: list[env.Reward] = []\n",
    "    # play the game\n",
    "    while not e.game_over():\n",
    "        for playerid, player in enumerate(players):\n",
    "            if player == actor:\n",
    "                if e.game_over_for(env.Player(playerid)):\n",
    "                    break\n",
    "                obs, action_probs, chosen_action, reward = actor.play(env.Player(playerid), e)\n",
    "                s_t += [obs]\n",
    "                p_t += [action_probs]\n",
    "                a_t += [chosen_action]\n",
    "                r_t += [reward]\n",
    "            else:\n",
    "                if e.game_over_for(env.Player(playerid)):\n",
    "                    continue\n",
    "                player.play(env.Player(playerid), e)\n",
    "        e.step()\n",
    "\n",
    "    # compute advantage and value\n",
    "    d_t = network.compute_advantage(actor.critic, s_t, r_t)\n",
    "    v_t = network.compute_value(r_t)\n",
    "\n",
    "    return s_t, a_t, p_t, r_t, d_t, v_t, actor_is_impostor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    for _ in range(TRAIN_EPOCHS):\n",
    "        crewmate_s_batch:list[env.Observation] = []\n",
    "        crewmate_a_batch:list[env.Action] = []\n",
    "        crewmate_p_batch:list[np.ndarray] = []\n",
    "        crewmate_d_batch:list[env.Advantage] = []\n",
    "        crewmate_v_batch:list[env.Value] = []\n",
    "        \n",
    "        impostor_s_batch:list[env.Observation] = []\n",
    "        impostor_a_batch:list[env.Action] = []\n",
    "        impostor_p_batch:list[np.ndarray] = []\n",
    "        impostor_d_batch:list[env.Advantage] = []\n",
    "        impostor_v_batch:list[env.Value] = []\n",
    "\n",
    "        # create actor player\n",
    "        crewmate_nn_player = player.ActorPlayer(crewmate_actor, crewmate_critic, step)\n",
    "        impostor_nn_player = player.ActorPlayer(impostor_actor, impostor_critic, step)\n",
    "\n",
    "        futures = []\n",
    "        for i in range(EPISODES_PER_AGENT):\n",
    "            is_impostor = (i % 2) == 0\n",
    "            actor_player = impostor_nn_player if is_impostor else crewmate_nn_player\n",
    "\n",
    "            others = []\n",
    "            for _ in range(3):\n",
    "                others.append(player.RandomPlayer())\n",
    "\n",
    "            # play the game\n",
    "            future = executor.submit(play, actor_player, is_impostor, others)\n",
    "            futures.append(future)\n",
    "\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            s_t, a_t, p_t, r_t, d_t, v_t, was_impostor = future.result()\n",
    "\n",
    "            # now update the minibatch\n",
    "            if was_impostor:\n",
    "                impostor_s_batch += s_t\n",
    "                impostor_a_batch += a_t\n",
    "                impostor_p_batch += p_t\n",
    "                impostor_d_batch += d_t\n",
    "                impostor_v_batch += v_t\n",
    "            else:\n",
    "                crewmate_s_batch += s_t\n",
    "                crewmate_a_batch += a_t\n",
    "                crewmate_p_batch += p_t\n",
    "                crewmate_d_batch += d_t\n",
    "                crewmate_v_batch += v_t\n",
    "\n",
    "            # statistics\n",
    "            if was_impostor:\n",
    "                impostor_reward_buf.append(np.sum(r_t))\n",
    "            else:\n",
    "                crewmate_reward_buf.append(np.sum(r_t))\n",
    "\n",
    "        crewmate_actor_losses, crewmate_critic_losses = network.train_ppo(\n",
    "            crewmate_actor,\n",
    "            crewmate_critic,\n",
    "            crewmate_actor_optimizer,\n",
    "            crewmate_critic_optimizer,\n",
    "            crewmate_s_batch,\n",
    "            crewmate_a_batch,\n",
    "            crewmate_p_batch,\n",
    "            crewmate_d_batch,\n",
    "            crewmate_v_batch\n",
    "        )\n",
    "\n",
    "        impostor_actor_losses, impostor_critic_losses = network.train_ppo(\n",
    "            impostor_actor,\n",
    "            impostor_critic,\n",
    "            impostor_actor_optimizer,\n",
    "            impostor_critic_optimizer,\n",
    "            impostor_s_batch,\n",
    "            impostor_a_batch,\n",
    "            impostor_p_batch,\n",
    "            impostor_d_batch,\n",
    "            impostor_v_batch\n",
    "        )\n",
    "\n",
    "        for actor_loss, critic_loss in zip(actor_losses, critic_losses):\n",
    "            writer.add_scalar('actor_loss', actor_loss, step)\n",
    "            writer.add_scalar('critic_loss', critic_loss, step)\n",
    "\n",
    "            if step % SUMMARY_STATS_INTERVAL == 0:\n",
    "                for opponent_name, rewards in rewards_vs.items():\n",
    "                    if len(rewards) > 50:\n",
    "                        avg_reward = np.array(rewards).mean()\n",
    "                        writer.add_scalar(f'reward_against_{opponent_name}', avg_reward, step)\n",
    "                        rewards_vs[opponent_name] = []\n",
    "\n",
    "            if step % MAKE_OPPONENT_INTERVAL == 0:\n",
    "                # create a new opponent\n",
    "                frozen_actor = copy.deepcopy(actor)\n",
    "                frozen_actor.eval()\n",
    "                frozen_actor.to(device)\n",
    "                frozen_critic = copy.deepcopy(critic)\n",
    "                frozen_critic.eval()\n",
    "                frozen_critic.to(device)\n",
    "                opponent_pool.append(player.ActorPlayer(frozen_actor, frozen_critic, step, env.PLAYER2))\n",
    "\n",
    "            if step % MODEL_SAVE_INTERVAL == 0:\n",
    "                # Save the neural net parameters to disk.\n",
    "                torch.save(actor.state_dict(), f\"{SUMMARY_DIR}/nn_model_ep_{step}_actor.ckpt\")\n",
    "                torch.save(critic.state_dict(), f\"{SUMMARY_DIR}/nn_model_ep_{step}_critic.ckpt\")\n",
    "            \n",
    "            step += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
